# -*- coding: utf-8 -*-
"""
ShirohaPet API æœåŠ¡
æä¾›èŠå¤©ã€é—®ç­”å’Œè§†è§‰ç†è§£æ¥å£
"""

from fastapi import FastAPI, Request
from datetime import datetime
from threading import Lock
from typing import List, Optional, Tuple
import uvicorn
import requests
import json
import torch
import platform
import sys
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from Shiroha.utils import get_config
import argparse

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except Exception:
    SKLEARN_AVAILABLE = False

# ç¡®ä¿æ ‡å‡†è¾“å‡ºä½¿ç”¨ UTF-8 ç¼–ç ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
if sys.stdout.encoding != 'utf-8':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


print("ğŸ–¥ï¸ åˆå§‹åŒ– PyTorch å¼•æ“...")
ENGINE = "torch"
if torch.cuda.is_available():
    DEVICE = "cuda"
    print("âœ… PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ CUDA åŠ é€Ÿ)")
else:
    DEVICE = "cpu"
    print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„ CUDA è®¾å¤‡ï¼Œä½¿ç”¨ CPU è¿›è¡Œæ¨ç†ï¼Œæ€§èƒ½å¯èƒ½è¾ƒå·®")

api = FastAPI()

adapter_path = "./models/Shiroha"
max_seq_length = 2048

DEFAULT_RAG_SOURCE = os.environ.get("RAG_SOURCE_FILE", "./rag_sources.txt")
DEFAULT_RAG_CHUNK_SIZE = int(os.environ.get("RAG_CHUNK_SIZE", "500"))
DEFAULT_RAG_CHUNK_OVERLAP = int(os.environ.get("RAG_CHUNK_OVERLAP", "100"))
DEFAULT_RAG_TOP_K = int(os.environ.get("RAG_TOP_K", "3"))
_rag_store = None
_rag_lock = Lock()


def _chunk_text(content: str, chunk_size: int, overlap: int) -> List[str]:
    """Split raw text into overlapping chunks."""
    normalized = content.replace("\r\n", "\n")
    paragraphs = [p.strip() for p in normalized.split("\n\n") if p.strip()]
    chunks: List[str] = []
    for paragraph in paragraphs:
        if len(paragraph) <= chunk_size:
            chunks.append(paragraph)
            continue
        start = 0
        while start < len(paragraph):
            end = min(len(paragraph), start + chunk_size)
            segment = paragraph[start:end].strip()
            if segment:
                chunks.append(segment)
            if end >= len(paragraph):
                break
            start = max(0, end - overlap)
    return chunks


class SimpleTextRAG:
    def __init__(self, chunks: List[str]):
        self.chunks = chunks
        if SKLEARN_AVAILABLE and chunks:
            self.vectorizer = TfidfVectorizer(stop_words="english")
            self.matrix = self.vectorizer.fit_transform(chunks)
        else:
            self.vectorizer = None
            self.matrix = None

    def _fallback_retrieve(self, query: str, top_k: int) -> List[str]:
        # Simple token overlap when sklearn is unavailable
        query_terms = set(query.lower().split())
        scored = []
        for idx, chunk in enumerate(self.chunks):
            tokens = set(chunk.lower().split())
            overlap = len(query_terms & tokens)
            if overlap:
                scored.append((overlap, idx))
        scored.sort(key=lambda item: item[0], reverse=True)
        return [self.chunks[idx] for _, idx in scored[:top_k]]

    def retrieve(self, query: str, top_k: int) -> List[str]:
        if not query or not self.chunks:
            return []
        if self.vectorizer is None or self.matrix is None:
            return self._fallback_retrieve(query, top_k)
        query_vec = self.vectorizer.transform([query])
        scores = cosine_similarity(query_vec, self.matrix)[0]
        ranked = scores.argsort()[::-1]
        results: List[str] = []
        for idx in ranked[:top_k]:
            if scores[idx] <= 0:
                continue
            results.append(self.chunks[idx])
        return results


def _load_rag_store(path: Optional[str]) -> Optional[SimpleTextRAG]:
    if not path or not os.path.exists(path):
        print(f"â„¹ï¸ RAG çŸ¥è¯†åº“æ–‡ä»¶ {path} ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            raw_text = f.read()
    except Exception as exc:
        print(f"âš ï¸ è¯»å– RAG æ–‡ä»¶å¤±è´¥: {exc}")
        return None
    if not raw_text.strip():
        print(f"â„¹ï¸ RAG æ–‡ä»¶ {path} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡åŠ è½½")
        return None
    chunk_size = max(128, DEFAULT_RAG_CHUNK_SIZE)
    overlap = min(DEFAULT_RAG_CHUNK_OVERLAP, chunk_size - 1)
    chunks = _chunk_text(raw_text, chunk_size=chunk_size, overlap=overlap)
    print(f"ğŸ“š RAG çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå…± {len(chunks)} æ®µ")
    return SimpleTextRAG(chunks)


def _resolve_rag_path() -> str:
    rag_path = DEFAULT_RAG_SOURCE
    try:
        cfg = get_config()
        rag_path = cfg.get("rag", {}).get("source_file") or rag_path
    except Exception as exc:
        print(f"âš ï¸ åŠ è½½é…ç½®æ—¶æ— æ³•è·å– RAG æ–‡ä»¶è·¯å¾„: {exc}")
    return rag_path


def _get_rag_store() -> Optional[SimpleTextRAG]:
    global _rag_store
    if _rag_store is not None:
        return _rag_store
    with _rag_lock:
        if _rag_store is not None:
            return _rag_store
        rag_path = _resolve_rag_path()
        _rag_store = _load_rag_store(rag_path)
    return _rag_store


def append_conversation_to_rag(prompt: str, reply: str) -> None:
    if not prompt and not reply:
        return
    rag_path = _resolve_rag_path()
    if not rag_path:
        return
    timestamp = get_current_time()
    entry = (
        "\n\n"
        f"[{timestamp}] USER INPUT:\n{prompt or '(empty)'}\n"
        f"[{timestamp}] ASSISTANT OUTPUT:\n{reply or '(empty)'}\n"
    )
    directory = os.path.dirname(rag_path)
    if directory and not os.path.exists(directory):
        try:
            os.makedirs(directory, exist_ok=True)
        except Exception as exc:
            print(f"âš ï¸ æ— æ³•åˆ›å»º RAG ç›®å½• {directory}: {exc}")
            return
    global _rag_store
    with _rag_lock:
        try:
            with open(rag_path, "a", encoding="utf-8") as rag_file:
                rag_file.write(entry)
        except Exception as exc:
            print(f"âš ï¸ æ— æ³•å†™å…¥ RAG æ–‡ä»¶ {rag_path}: {exc}")
            return
        _rag_store = _load_rag_store(rag_path)


def augment_prompt_with_rag(prompt: str) -> Tuple[str, List[str]]:
    store = _get_rag_store()
    if not prompt or store is None:
        return prompt, []
    top_k = max(1, DEFAULT_RAG_TOP_K)
    snippets = store.retrieve(prompt, top_k=top_k)
    if not snippets:
        return prompt, []
    context = "\n\n".join(f"[{idx + 1}] {snippet.strip()}" for idx, snippet in enumerate(snippets))
    augmented = (
        "You have access to the following reference notes pulled from a trusted knowledge file. "
        "Use them when helpful and cite only what is supported.\n"
        f"{context}\n\n"
        f"User question: {prompt}"
    )
    return augmented, snippets


def load_model_and_tokenizer():
    print(f"ğŸ“‚ æ¨¡å‹åŠ è½½è·¯å¾„: {adapter_path}")
    print(f"âš™ï¸ æ¨ç†å¼•æ“: {ENGINE} | è®¡ç®—è®¾å¤‡: {DEVICE}")


    print("ğŸ”§ æ­£åœ¨åŠ è½½ PyTorch LoRA æ¨¡å‹...")

    try:
        print("ğŸ”„ æ­£åœ¨å‡†å¤‡åŸºç¡€æ¨¡å‹å’Œ LoRA é€‚é…å™¨...")
        adapter_config_path = os.path.join(adapter_path, "adapter_config.json")
        if not os.path.exists(adapter_config_path):
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ°é€‚é…å™¨é…ç½®æ–‡ä»¶ {adapter_config_path}")
            print("ğŸ’¡ è¯·è¿è¡Œ download.py ä»¥ä¸‹è½½åŸºç¡€æ¨¡å‹ä¸ LoRA é€‚é…å™¨")
            exit(1)

        with open(adapter_config_path, "r", encoding="utf-8") as f:
            adapter_config = json.load(f)

        base_model_path = adapter_config.get("base_model_name_or_path")
        if not base_model_path:
            print("âŒ ä¸¥é‡é”™è¯¯ï¼šé€‚é…å™¨é…ç½®ç¼ºå°‘ base_model_name_or_path")
            exit(1)
        if not os.path.exists(base_model_path):
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šåŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
            print("ğŸ’¡ è¯·ç¡®è®¤ Qwen3-14B æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½å¹¶ä¸ adapter_config.json ä¸­çš„è·¯å¾„ä¸€è‡´")
            exit(1)

        torch_dtype = torch.float16 if DEVICE == "cuda" else torch.float32
        device_map = "cuda"
        print("åœ¨cudaä¸ŠåŠ è½½æ¨¡å‹ï¼Œç”¨nvidia-smiç›‘æ§æ˜¾å­˜çŠ¶æ€")

        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=True,
        )

        print(f"ğŸ¯ æ­£åœ¨åº”ç”¨ LoRA é€‚é…å™¨: {adapter_path}")
        model = PeftModel.from_pretrained(
            base_model,
            adapter_path,
            device_map=device_map,
        )

        model.eval()

        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "left"

        print("âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"   ğŸ“ åŸºç¡€æ¨¡å‹: {base_model_path}")
        print(f"   ğŸ“ é€‚é…å™¨: {adapter_path}")
        print(f"   ğŸ·ï¸ æ¨ç†è®¾å¤‡: {DEVICE}")
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½ PyTorch LoRA æ¨¡å‹ï¼")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        print()
        print("ğŸ” å¯èƒ½çš„åŸå› ï¼š")
        print("1. LoRA æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´")
        print("2. ç¼ºå°‘å¿…éœ€çš„ PyTorch ä¾èµ–")
        print()
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
        print("é‡æ–°è¿è¡Œ download.py ç¡®ä¿ LoRA æ–‡ä»¶æ­£ç¡®ä¸‹è½½")
        print()
        print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šåº”ç”¨éœ€è¦ LoRA æ¨¡å‹æ‰èƒ½è¿è¡Œ")
        exit(1)

    return model, tokenizer


# è¾…åŠ©å‡½æ•°ï¼šè·å–å½“å‰æ—¶é—´
def get_current_time():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•è¯·æ±‚æ—¥å¿—
def log_request(prompt):
    print(f'ğŸ“¥ [{get_current_time()}] æ”¶åˆ°ç”¨æˆ·è¯·æ±‚: {prompt}')


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•å“åº”æ—¥å¿—
def log_response(response):
    print(f'ğŸ“¤ [{get_current_time()}] ç”Ÿæˆæœ€ç»ˆå›å¤: {response}')


# è¾…åŠ©å‡½æ•°ï¼šè§£æè¯·æ±‚
def parse_request(json_post_list):
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    return prompt, history


# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæ ‡å‡†å“åº”
def create_response(response_text, history, status=200):
    time = get_current_time()
    return {
        "response": response_text,
        "history": history,
        "status": status,
        "time": time
    }



def call_openrouter_api(config, api_key, model, messages, image_url=None, max_tokens=2048):
    """è°ƒç”¨ OpenRouter API"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://Shiroha-pet.local",
        "X-Title": "ShirohaPet"
    }

    # å¤„ç†å›¾åƒè¾“å…¥ - æŒ‰ç…§ OpenRouter å®˜æ–¹æ–‡æ¡£æ ¼å¼
    if image_url:
        # å¦‚æœæœ‰å›¾åƒï¼Œå°†æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä¿®æ”¹ä¸ºåŒ…å«å›¾åƒ
        for message in reversed(messages):
            if message['role'] == 'user':
                if isinstance(message['content'], str):
                    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå®˜æ–¹æ–‡æ¡£è¦æ±‚çš„æ•°ç»„æ ¼å¼
                    message['content'] = [
                        {"type": "text", "text": message['content']},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]
                elif isinstance(message['content'], list):
                    # å¦‚æœå·²ç»æ˜¯æ•°ç»„æ ¼å¼ï¼Œç›´æ¥æ·»åŠ å›¾åƒ
                    message['content'].append({"type": "image_url", "image_url": {"url": image_url}})
                break

    data = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": max_tokens
    }

    # ä»é…ç½®ä¸­è·å– OpenRouter åœ°å€ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
    endpoint_url = config.get('endpoints', {}).get('openrouter', "https://openrouter.ai/api/v1/chat/completions")
    response = requests.post(endpoint_url, headers=headers, json=data)
    response.raise_for_status()
    return response.json()


@api.post("/chat")
async def create_chat(request: Request):
    print("[DEBUG] Create chat is called")
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    prompt = prompt or ""
    log_request(prompt)
    history = history or []
    history_with_user = history + [{'role': 'user', 'content': prompt}]

    augmented_prompt, rag_chunks = augment_prompt_with_rag(prompt)
    if rag_chunks:
        print(f"ğŸ“š RAG å‘½ä¸­ {len(rag_chunks)} æ®µä¸Šä¸‹æ–‡å‚ä¸å›ç­”")

    model_history = history + [{'role': 'user', 'content': augmented_prompt}]

    print(f"ğŸ’¬ ä½¿ç”¨ {ENGINE.upper()} å¼•æ“è¿›è¡Œæ¨ç†...")
    print(f"ğŸ“Š æœ€å¤§ç”Ÿæˆé•¿åº¦: {json_post_list.get('max_new_tokens', 2048)} tokens")
    
    text = tokenizer.apply_chat_template(
        model_history,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )
    print("âœ… èŠå¤©æ¨¡æ¿åº”ç”¨å®Œæˆ")

    max_new_tokens = int(json_post_list.get('max_new_tokens', 2048))
    max_new_tokens = max(1, max_new_tokens)
    temperature = float(json_post_list.get('temperature', 0.7))
    top_p = float(json_post_list.get('top_p', 0.9))
    top_p = max(0.01, min(top_p, 1.0))

    # æ¨ç†
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
    encoded = tokenizer(
        text,
        return_tensors="pt",
    )
    encoded = {k: v.to(DEVICE) for k, v in encoded.items()}
    generation_kwargs = {
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "temperature": max(0.01, temperature),
        "top_p": top_p,
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.eos_token_id,
    }
    with torch.no_grad():
        generated = model.generate(
            **encoded,
            **generation_kwargs,
        )
    generated_tokens = generated[0, encoded["input_ids"].shape[-1]:]
    reply = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

    print(f"âœ… å›å¤ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(reply)} å­—ç¬¦)")

    history_with_user.append({"role": "assistant", "content": reply})
    append_conversation_to_rag(prompt, reply)

    log_response(reply)
    return create_response(reply, history_with_user)

@api.post("/qwen3")
async def create_qwen3_chat(request: Request):
    """
    Robust qwen3 endpoint:
    - If enable_qwen3 is False: return the latest assistant reply from history (if any).
      Try multiple fallbacks: json field 'reply', 'message', 'content', or last assistant in history.
    - Add lots of debug printing so we can see what the client actually sent.
    """
    config = get_config()
    json_post_list = await request.json()

    # Debug: dump what client sent (important to inspect)
    try:
        print("---- /qwen3 REQUEST BODY DUMP ----")
        print(json.dumps(json_post_list, ensure_ascii=False, indent=2))
        print("---- end request dump ----")
    except Exception:
        print("(/qwen3) cannot pretty-print request body")

    # Parse request with your parser if available
    try:
        prompt, history = parse_request(json_post_list)
    except Exception as e:
        print(f"/qwen3 parse_request failed: {e}")
        # best effort: try to extract history from common keys
        history = json_post_list.get("history") or json_post_list.get("messages") or []
        prompt = json_post_list.get("prompt") or json_post_list.get("text") or ""

    # Debug print of parsed history
    print("/qwen3 parsed prompt:", repr(prompt))
    print(f"/qwen3 parsed history length: {len(history)}")
    for i, msg in enumerate(history[-8:], start=max(0, len(history)-8)):
        print(f"  history[{i}] = ({msg.get('role')}) {repr(msg.get('content'))}")

    if prompt != "":
        history = history + [{'role': 'assistant', 'content': prompt}]

    # If Qwen disabled -> short-circuit and return a safe fallback
    if not config.get("enable_qwen3", True):
        print("ğŸš« Qwen3 å·²ç¦ç”¨ï¼šå‡†å¤‡è¿”å› LoRA çš„æœ€åæœ‰æ•ˆå›å¤ï¼ˆæ‰§è¡Œå¤šç§ fallbackï¼‰")

        # 1) Try explicit 'reply' field from client body
        reply_candidates = []
        if isinstance(json_post_list, dict):
            for key in ("reply", "message", "content", "text"):
                val = json_post_list.get(key)
                if isinstance(val, str) and val.strip():
                    reply_candidates.append(val.strip())

        # 2) Last non-empty assistant in parsed history
        for msg in reversed(history):
            if msg.get("role") == "assistant":
                c = (msg.get("content") or "").strip()
                if c:
                    reply_candidates.append(c)
                    break

        # 3) Last non-empty user message as ultimate fallback
        if not reply_candidates:
            for msg in reversed(history):
                if msg.get("role") == "user":
                    c = (msg.get("content") or "").strip()
                    if c:
                        reply_candidates.append(c)
                        break

        # 4) final fallback -> empty string (do not use "â€¦â€¦")
        final_reply = reply_candidates[0] if reply_candidates else ""

        print(f"/qwen3 final_reply chosen (len={len(final_reply)}): {repr(final_reply)}")

        # Return in the same response shape your front-end expects (use create_response)
        return create_response(final_reply, history)

    # If Qwen3 enabled -> regular flow (kept from your original code)
    api_key = config.get('openrouter_api_key', '')
    endpoint_url = config.get('server', {}).get('qwen3', '')

    if "openrouter.ai" in endpoint_url and api_key.strip():
        print("ğŸŒ ä½¿ç”¨ OpenRouter è°ƒç”¨ Qwen3...")
        try:
            result = call_openrouter_api(
                config,
                api_key,
                "qwen/qwen3-235b-a22b",
                history,
                max_tokens=4096
            )
            final_response = result['choices'][0]['message']['content']
        except Exception as e:
            error_msg = f"OpenRouter API é”™è¯¯: {str(e)}"
            log_response(error_msg)
            return create_response(error_msg, history, status=500)
    else:
        print(f"ğŸ  ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ ({endpoint_url}) è°ƒç”¨ Qwen3...")
        try:
            response = requests.post(
                f"{endpoint_url}/api/chat",
                json={
                    "model": "qwen3:14b",
                    "messages": history,
                    "stream": False,
                    "options": {"keep_alive": -1}
                },
                timeout=60
            )
            response.raise_for_status()
            final_response = response.json()['message']['content']
        except Exception as e:
            print(f"âŒ æœ¬åœ° API è°ƒç”¨å¤±è´¥: {e}")
            raise

    history.append({'role': 'assistant', 'content': final_response})
    log_response(final_response)
    return create_response(final_response, history)

@api.post("/qwenvl")
async def create_qwenvl_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)

    if "image" in json_post_list:
        image_url = json_post_list.get('image')
        print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒè¾“å…¥: {image_url[:100]}...")
        history = history + [{'role': 'user', 'content': prompt, 'images': [image_url]}]
    else:
        print("ğŸ“ çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆæ— å›¾åƒè¾“å…¥ï¼‰")
        history = history + [{'role': 'user', 'content': prompt}]

    config = get_config()
    api_key = config.get('openrouter_api_key', '')
    endpoint_url = config.get('server', {}).get('qwenvl', '')
    image_url_for_api = json_post_list.get('image') if "image" in json_post_list else None

    # ä»…å½“ endpoint æŒ‡å‘ openrouter ä¸” API key å­˜åœ¨æ—¶ï¼Œæ‰ä½¿ç”¨ OpenRouter
    if "openrouter.ai" in endpoint_url and api_key.strip():
        print(f"ğŸŒ æ£€æµ‹åˆ° qwenvl endpoint æŒ‡å‘ OpenRouterï¼Œä½¿ç”¨ API Key è¿›è¡Œè°ƒç”¨...")
        try:
            result = call_openrouter_api(
                config,
                api_key,
                "qwen/qwen-2.5-vl-7b-instruct",
                history,
                image_url=image_url_for_api
            )
            final_response = result['choices'][0]['message']['content']
            print("âœ… OpenRouter è§†è§‰ API è°ƒç”¨æˆåŠŸ")
        except Exception as e:
            error_msg = f"OpenRouter API é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            log_response(error_msg)
            return create_response(error_msg, history, status=500)
    else:
        # ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ (Ollama æˆ–å…¶ä»–)
        print(f"ğŸ  ä½¿ç”¨æœ¬åœ°ç«¯ç‚¹ ({endpoint_url}) è¿›è¡Œè°ƒç”¨...")
        try:
            response = requests.post(
                f"{endpoint_url}/api/chat",
                json={"model": "qwen2.5vl:7b", "messages": history,
                      "stream": False, "options": {"keep_alive": -1}},
            )
            response.raise_for_status()
            final_response = response.json()['message']['content']
            print("âœ… æœ¬åœ°è§†è§‰ API è°ƒç”¨æˆåŠŸ")
        except requests.exceptions.RequestException as e:
            print(f"âŒ è°ƒç”¨æœ¬åœ°è§†è§‰ API æ—¶å‡ºé”™: {e}")
            raise

    history = history + [{'role': 'assistant', 'content': final_response}]
    log_response(final_response)
    return create_response(final_response, history)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=28565)
    args = parser.parse_args()

    print("=" * 60)
    print("ğŸš€ ShirohaPet API æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)
    
    model, tokenizer = load_model_and_tokenizer()
    
    print("=" * 60)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:{args.port}")
    print("=" * 60)

    uvicorn.run(api, host='0.0.0.0', port=args.port, workers=1)